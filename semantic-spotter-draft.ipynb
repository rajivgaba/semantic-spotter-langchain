{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "\n",
    "! pip install -qU langchain-community pymupdf\n",
    "! pip install -qU langchain_huggingface\n",
    "! pip install -qU sentence-transformers\n",
    "! pip install -qU langchain-perplexity\n",
    "! pip install -qU faiss-cpu\n",
    "! pip install gradio\n",
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c604d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RajivGaba/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, CSVLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import CrossEncoder, util\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b9abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the key for LLM API\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['PERPLEXITY_API_KEY'] = userdata.get('PPLX_API_KEY_2')\n",
    "except:\n",
    "    import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    os.environ['PERPLEXITY_API_KEY'] = os.getenv('PPLX_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6ea496",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path' : '/Users/RajivGaba/aiml_projects/Semantic Spotter/Data/',\n",
    "    'chunk_size' : 512,\n",
    "    'chunk_overlap' : 80,\n",
    "    'vector_store_name' : \"faiss_myntra_db\",\n",
    "    'embedding_model' : 'all-MiniLM-L6-v2',\n",
    "    'refresh_vector_store' : 'N',\n",
    "    'PPLX_API_KEY' : os.getenv('PERPLEXITY_API_KEY'),\n",
    "    'domain' : 'fashion',\n",
    "    'chat_model' : \"sonar-pro\",\n",
    "    'rerank_model' : 'BAAI/bge-reranker-base'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca535a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from kaggle using kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# api.dataset_download_files('promptcloud/myntra-e-commerce-product-data-november-2023', path='./data/', unzip=True)\n",
    "# api.dataset_download_files(\"ronakbokaria/myntra-products-dataset\", path='./data/', unzip=True)\n",
    "api.dataset_download_files(\"djagatiya/myntra-fashion-product-dataset\", path='./data/', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67bc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reusable functions\n",
    "\n",
    "def get_data_chunks(folder_path):\n",
    "  \"\"\"\n",
    "  This function will create chunks from the files present in the dataset.\n",
    "  This takes directory where the files exists. Basis the type of file i.e.\n",
    "  PDF, CSV or text, a loader is initialised and chunks are created from\n",
    "  content of the data\n",
    "  \"\"\"\n",
    "  # loader = PyMuPDFLoader(pdf_file)\n",
    "  # documents = loader.load()\n",
    "\n",
    "  all_documents = []\n",
    "\n",
    "  # Loop through all files in the folder\n",
    "  for file_path in folder_path.iterdir():\n",
    "      if file_path.suffix.lower() == \".pdf\":\n",
    "          loader = PyMuPDFLoader(str(file_path))\n",
    "      elif file_path.suffix.lower() == \".csv\":\n",
    "          loader = CSVLoader(str(file_path))\n",
    "      elif file_path.suffix.lower() == \".txt\":\n",
    "          loader = TextLoader(str(file_path))\n",
    "      else:\n",
    "          continue  # Skip unsupported file types\n",
    "\n",
    "      # Load and append documents\n",
    "      documents = loader.load()\n",
    "      all_documents.extend(documents)\n",
    "\n",
    "  # chunking/splitting\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=config['chunk_size'],\n",
    "      chunk_overlap=config['chunk_overlap'],\n",
    "      strip_whitespace=True,\n",
    "      separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "  )\n",
    "  text_chunks = text_splitter.split_documents(documents=all_documents)\n",
    "  return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808bad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_model():\n",
    "  embedding_model = HuggingFaceEmbeddings(\n",
    "      model_name=config['embedding_model'],\n",
    "      show_progress=True,\n",
    "      multi_process=True,\n",
    "      model_kwargs={'device': 'mps'}\n",
    "  )\n",
    "  return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290f41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(text_chunks, embedding_model):\n",
    "  if config['refresh_vector_store'] == 'Y' or not os.path.exists(config['vector_store_name']):\n",
    "      vector_store = FAISS.from_documents(text_chunks, embedding_model)\n",
    "      vector_store.save_local(config['vector_store_name'])\n",
    "  else:\n",
    "      vector_store = FAISS.load_local(config['vector_store_name'], embedding_model, allow_dangerous_deserialization=True)\n",
    "  return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebb800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_client():\n",
    "  return ChatPerplexity(\n",
    "      temperature=0,\n",
    "      pplx_api_key=config['PPLX_API_KEY'], # Pass the API key explicitly\n",
    "      model=config['chat_model']\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbe7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(top_k=5):\n",
    "  retriever = vector_store.as_retriever(search_kwargs={'k': top_k})\n",
    "  return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762f2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reranked_query_results(query):\n",
    "  model = HuggingFaceCrossEncoder(model_name=config['rerank_model'])\n",
    "\n",
    "  compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "\n",
    "  compression_retriever = ContextualCompressionRetriever(\n",
    "      base_compressor=compressor,\n",
    "      base_retriever=get_retriever()\n",
    "  )\n",
    "\n",
    "  compressed_docs = compression_retriever.invoke(query)\n",
    "\n",
    "  return compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2926668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(user_question, history):\n",
    "\n",
    "  print(f\"{history}\")\n",
    "\n",
    "  # Retrieve relevant documents from the vector store based on the user's question.\n",
    "  retrieved_documents = get_reranked_query_results(user_question)\n",
    "\n",
    "  # Extract the page content from the retrieved documents and join them into a single context string.\n",
    "  formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_documents)\n",
    "\n",
    "  # Call the large language model with the user's question and the formatted context.\n",
    "  answer = get_llm_response(user_question, formatted_context, config['domain'])\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "959c490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_history_pipeline(new_user_message, gradio_history, llm , retriever, system_message):\n",
    "    # --- STEP 1: Load Gradio history into LangChain Memory ---\n",
    "    # Gradio sends history in the format: [[user_msg, bot_msg], [user_msg, bot_msg], ...]\n",
    "    # We must reset the memory and rebuild it for each new call,\n",
    "    # as Gradio manages the state outside of LangChain's memory object.\n",
    "\n",
    "    # Clear memory from previous (potentially different user) sessions\n",
    "    memory.clear()\n",
    "\n",
    "    # Convert Gradio format to LangChain Messages and load into memory\n",
    "    for message_pair in gradio_history:\n",
    "        # message_pair[0] is User message, message_pair[1] is Assistant message\n",
    "        user_text = message_pair[0]\n",
    "        ai_text = message_pair[1]\n",
    "\n",
    "        # Add the conversation turn to the memory object\n",
    "        memory.chat_memory.add_user_message(user_text)\n",
    "        memory.chat_memory.add_ai_message(ai_text)\n",
    "\n",
    "\n",
    "    # --- STEP 2: RAG Logic (Retrieve context using the new question AND the history) ---\n",
    "    # Your current retrieval logic needs the new question (new_user_message)\n",
    "    # AND the context from the conversation history to perform better retrieval.\n",
    "\n",
    "    # Create the full query string by joining the last few messages\n",
    "    # This helps the retriever find context relevant to the full conversation.\n",
    "    if gradio_history:\n",
    "        last_turn = f\"User: {gradio_history[-1][0]} | Assistant: {gradio_history[-1][1]}\"\n",
    "        full_query = f\"{last_turn} | New Question: {new_user_message}\"\n",
    "    else:\n",
    "        full_query = new_user_message\n",
    "\n",
    "    # Run your vector store retrieval using 'full_query'\n",
    "    final_retrieved_docs = retriever.invoke(full_query)\n",
    "\n",
    "    # Format the retrieved documents into the context string\n",
    "    formatted_context = \"\\n\".join([doc.page_content for doc in final_retrieved_docs])\n",
    "\n",
    "    # NOTE: You'll need to update your final prompt template to include {context}\n",
    "    # and pass the formatted_context into the invoke call.\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",{system_message},), MessagesPlaceholder(variable_name=\"chat_history\"), (\"human\", \"{full_query}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # --- STEP 3: Invoke the Chain ---\n",
    "    # The chain automatically pulls 'chat_history' from memory,\n",
    "    # and inserts 'input' and 'context' (if you update the prompt)\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke(\n",
    "\n",
    "    )\n",
    "\n",
    "    # The final memory state is now updated internally by the chain.\n",
    "    # The Gradio interface will update its displayed history based on the returned string.\n",
    "    return response['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c32fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = \"i am looking for an outfit for hill station vacation\"\n",
    "# Initialize LLM chat model\n",
    "\n",
    "llm = create_chat_client()\n",
    "\n",
    "# 1. Define the Memory Component\n",
    "# The memory stores the history (user/assistant messages)\n",
    "# It uses 'chat_history' as the key to insert into the prompt template\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 2. Define the Prompt Template\n",
    "# Use MessagesPlaceholder to tell LangChain where to inject the list of messages\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful fashion assistant. Use the following context to answer the user's question. If you don't know the answer, say so. Keep the conversation flowing.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Define a simple Chain (or your RAG chain wrapper)\n",
    "# For this simplified example, we'll use a basic LLMChain\n",
    "chain = prompt | llm\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"query\": query,\n",
    "        \"chat_history\": []\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9353d8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a hill station vacation, the best outfit combines **layered clothing, comfortable bottoms, and sturdy footwear** to adapt to changing temperatures and terrain[1][2]. Start with a **breathable base layer** like a cotton shirt or thermal top, add a **sweater or cardigan** for warmth, and finish with a **jacket or coat** for chilly mornings and evenings[1][2].\n",
      "\n",
      "**Key outfit elements:**\n",
      "- **Trousers, hiking pants, or joggers** for comfort and mobility[1].\n",
      "- **Sweaters, hoodies, or light wool pullovers** for warmth[1][2].\n",
      "- **Denim jeans or shorts** paired with flannel shirts or blouses for a casual look[2].\n",
      "- **Dresses or maxi dresses** with cardigans or jackets for a versatile, stylish option (especially in summer)[1][3].\n",
      "- **Sturdy boots or hiking shoes** for walking and exploring; sneakers or espadrilles for relaxed outings[1][2].\n",
      "- **Accessories** like a wide-brimmed hat, scarf, and a functional backpack for sun protection and convenience[2].\n",
      "\n",
      "**Color and style tips:**\n",
      "- **Bright colors** (yellow, orange, blue) stand out in photos and complement the natural scenery[1].\n",
      "- **Nature-inspired prints** (floral, leafy) blend well with the surroundings[2][3].\n",
      "- **Layering** is essentialâ€”hill stations can have unpredictable weather, so being able to add or remove layers is key[1][2].\n",
      "\n",
      "**Sample outfit idea:**\n",
      "- Base: **Thermal top or cotton T-shirt**\n",
      "- Middle: **Sweater or fleece**\n",
      "- Outer: **Waterproof jacket**\n",
      "- Bottom: **Trousers or jeans**\n",
      "- Footwear: **Ankle boots or hiking shoes**\n",
      "- Accessories: **Scarf, hat, sunglasses, and a backpack**\n",
      "\n",
      "This approach ensures you stay **comfortable, stylish, and prepared** for both sightseeing and outdoor activities during your hill station vacation[1][2][3].\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b98630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunked_data = []\n",
    "folder_path = Path(config['data_path'])\n",
    "# folder_path = Path(\"/content/data/\")\n",
    "\n",
    "# Step 1: Generate chunks from the dataset\n",
    "if config['refresh_vector_store'] == 'Y':\n",
    "    chunked_data = get_data_chunks(folder_path)\n",
    "\n",
    "# Step 2: Create embeddings and put them into a vector store\n",
    "embedding_model = get_embeddings_model()\n",
    "vector_store = create_vector_store(chunked_data, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f58efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reranked_query_results(\"i need a new outfit\")\n",
    "llm = create_chat_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aa3b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/6035x_495mg87nyn6pl05mp40000gq/T/ipykernel_11993/2186140973.py:4: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=500),\n",
      "/Users/RajivGaba/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/chat_interface.py:323: UserWarning: The type of the gr.Chatbot does not match the type of the gr.ChatInterface.The type of the gr.ChatInterface, 'messages', will be used.\n",
      "  warnings.warn(\n",
      "/Users/RajivGaba/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/utils.py:1052: UserWarning: Expected 5 arguments for function <function rag_with_history_pipeline at 0x1788fe7a0>, received 2.\n",
      "  warnings.warn(\n",
      "/Users/RajivGaba/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/utils.py:1056: UserWarning: Expected at least 5 arguments for function <function rag_with_history_pipeline at 0x1788fe7a0>, received 2.\n",
      "  warnings.warn(\n",
      "/Users/RajivGaba/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/utils.py:1052: UserWarning: Expected 5 arguments for function <function rag_with_history_pipeline at 0x1788fe700>, received 2.\n",
      "  warnings.warn(\n",
      "/Users/RajivGaba/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/utils.py:1056: UserWarning: Expected at least 5 arguments for function <function rag_with_history_pipeline at 0x1788fe700>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m iface = gr.ChatInterface(\n\u001b[32m      2\u001b[39m     fn=rag_with_history_pipeline,\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mtype\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     cache_examples=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43miface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshare\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/blocks.py:2734\u001b[39m, in \u001b[36mBlocks.launch\u001b[39m\u001b[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[39m\n\u001b[32m   2732\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2733\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m         share_url = \u001b[43mnetworking\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_tunnel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_port\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshare_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshare_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshare_server_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshare_server_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshare_server_tls_certificate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshare_server_tls_certificate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2741\u001b[39m         parsed_url = urlparse(share_url)\n\u001b[32m   2742\u001b[39m         \u001b[38;5;28mself\u001b[39m.share_url = urlunparse(\n\u001b[32m   2743\u001b[39m             (\u001b[38;5;28mself\u001b[39m.share_server_protocol,) + parsed_url[\u001b[32m1\u001b[39m:]\n\u001b[32m   2744\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/networking.py:59\u001b[39m, in \u001b[36msetup_tunnel\u001b[39m\u001b[34m(local_host, local_port, share_token, share_server_address, share_server_tls_certificate)\u001b[39m\n\u001b[32m     50\u001b[39m     remote_port = \u001b[38;5;28mint\u001b[39m(remote_port)\n\u001b[32m     51\u001b[39m tunnel = Tunnel(\n\u001b[32m     52\u001b[39m     remote_host,\n\u001b[32m     53\u001b[39m     remote_port,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     share_server_tls_certificate,\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m address = \u001b[43mtunnel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m address\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/tunneling.py:114\u001b[39m, in \u001b[36mTunnel.start_tunnel\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart_tunnel\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.download_binary()\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28mself\u001b[39m.url = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_start_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBINARY_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.url\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/tunneling.py:154\u001b[39m, in \u001b[36mTunnel._start_tunnel\u001b[39m\u001b[34m(self, binary)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mself\u001b[39m.proc = subprocess.Popen(\n\u001b[32m    151\u001b[39m     command, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n\u001b[32m    152\u001b[39m )\n\u001b[32m    153\u001b[39m atexit.register(\u001b[38;5;28mself\u001b[39m.kill)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_url_from_tunnel_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/Semantic Spotter/.venv/lib/python3.13/site-packages/gradio/tunneling.py:169\u001b[39m, in \u001b[36mTunnel._read_url_from_tunnel_stream\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTUNNEL_ERROR_MESSAGE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlog_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m url == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# check for timeout and log\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m - start_timestamp >= TUNNEL_TIMEOUT_SECONDS:\n\u001b[32m    170\u001b[39m         _raise_tunnel_error()\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# noqa: S101\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "iface = gr.ChatInterface(\n",
    "    fn=rag_with_history_pipeline,\n",
    "    type = 'messages',\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    "    textbox=gr.Textbox(placeholder=\"Help me find a formal shirt..\", container=False, scale=7),\n",
    "    title=\"ClosetAI - Fashion Studio\",\n",
    "    description=\"Ask ClosetAI anything about fashion products on Mytra\",\n",
    "    theme=\"glass\",\n",
    "    # examples=[\"Blazers for men\", \"Party dresses for women\", \"Athleisure\" , \"Sports attire\" ],\n",
    "    cache_examples=True,\n",
    ")\n",
    "iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d660d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
