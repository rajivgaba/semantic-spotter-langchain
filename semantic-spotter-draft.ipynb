{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "\n",
    "! pip install -qU langchain-community pymupdf\n",
    "! pip install -qU langchain_huggingface\n",
    "! pip install -qU sentence-transformers\n",
    "! pip install -qU langchain-perplexity\n",
    "! pip install -qU faiss-cpu\n",
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c604d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RajivGaba/aiml_projects/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, CSVLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import CrossEncoder, util\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "import gradio as gr\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee55b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import key from local env\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.environ['PERPLEXITY_API_KEY'] = os.getenv('PPLX_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0096a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the key for LLM API : when using google colab\n",
    "# os.environ['PREPLEXITY_API_KEY'] = userdata.get('PREPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453649c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base configuration values\n",
    "\n",
    "config = {\n",
    "    'data_path' : '/Users/RajivGaba/aiml_projects/Semantic Spotter/Data/',\n",
    "    'chunk_size' : 512,\n",
    "    'chunk_overlap' : 80,\n",
    "    'vector_store_name' : \"faiss_vector\",\n",
    "    'embedding_model' : 'all-MiniLM-L6-v2',\n",
    "    'refresh_vector_store' : 'N',\n",
    "    'PPLX_API_KEY' : os.environ['PERPLEXITY_API_KEY'],\n",
    "    'domain' : 'fashion',\n",
    "    'chat_model' : \"sonar-pro\",\n",
    "    'rerank_model' : 'BAAI/bge-reranker-base'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca535a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/ronakbokaria/myntra-products-dataset\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from kaggle using kaggle API\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# api.dataset_download_files('promptcloud/myntra-e-commerce-product-data-november-2023', path='./data/', unzip=True)\n",
    "api.dataset_download_files(\"ronakbokaria/myntra-products-dataset\", path='./data/', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba29c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reusable functions\n",
    "\n",
    "def get_data_chunks(folder_path):\n",
    "  \"\"\"\n",
    "  This function will create chunks from the files present in the dataset.\n",
    "  This takes directory where the files exists. Basis the type of file i.e.\n",
    "  PDF, CSV or text, a loader is initialised and chunks are created from\n",
    "  content of the data\n",
    "  \"\"\"\n",
    "  # loader = PyMuPDFLoader(pdf_file)\n",
    "  # documents = loader.load()\n",
    "\n",
    "  all_documents = []\n",
    "\n",
    "  # Loop through all files in the folder\n",
    "  for file_path in folder_path.iterdir():\n",
    "      if file_path.suffix.lower() == \".pdf\":\n",
    "          loader = PyMuPDFLoader(str(file_path))\n",
    "      elif file_path.suffix.lower() == \".csv\":\n",
    "          loader = CSVLoader(str(file_path))\n",
    "      elif file_path.suffix.lower() == \".txt\":\n",
    "          loader = TextLoader(str(file_path))\n",
    "      else:\n",
    "          continue  # Skip unsupported file types\n",
    "\n",
    "      # Load and append documents\n",
    "      documents = loader.load()\n",
    "      all_documents.extend(documents)\n",
    "\n",
    "  # chunking/splitting\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=config['chunk_size'],\n",
    "      chunk_overlap=config['chunk_overlap'],\n",
    "      strip_whitespace=True,\n",
    "      separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "  )\n",
    "  text_chunks = text_splitter.split_documents(documents=all_documents)\n",
    "  return text_chunks\n",
    "\n",
    "def get_embeddings_model():\n",
    "  embedding_model = HuggingFaceEmbeddings(\n",
    "      model_name=config['embedding_model'],\n",
    "      show_progress=True,\n",
    "      multi_process=False,\n",
    "      model_kwargs={'device': 'mps'}\n",
    "  )\n",
    "  return embedding_model\n",
    "\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "  if config['refresh_vector_store'] == 'Y' or not os.path.exists(config['vector_store_name']):\n",
    "      vector_store = FAISS.from_documents(text_chunks, embedding_model)\n",
    "      vector_store.save_local(config['vector_store_name'])\n",
    "  else:\n",
    "      vector_store = FAISS.load_local(config['vector_store_name'], embedding_model, allow_dangerous_deserialization=True)\n",
    "  return vector_store\n",
    "\n",
    "def get_cross_encoder_score(query, results):\n",
    "  cross_encoder = CrossEncoder(config['rerank_model'])\n",
    "  for i, res in enumerate(results):\n",
    "      ce_score = cross_encoder.predict([query, res.page_content])\n",
    "      print(ce_score)\n",
    "\n",
    "def create_chat_client():\n",
    "  return ChatPerplexity(\n",
    "      temperature=0,\n",
    "      pplx_api_key=config['PPLX_API_KEY'],\n",
    "      model=config['chat_model']\n",
    "  )\n",
    "\n",
    "def get_retriever(top_k=5):\n",
    "  retriever = vector_store.as_retriever(search_kwargs={'k': top_k})\n",
    "  return retriever\n",
    "\n",
    "def get_llm_response(query, results, domain):\n",
    "  system = \"\"\"\n",
    "  You are a helpful assistant in {domain} domain.\n",
    "  Using the information contained in the context, give a comprehensive answer to the question.\n",
    "  Respond only to the question asked, response should be concise and relevant to the question.\n",
    "  Provide the number of the source document when relevant.\n",
    "  If the answer cannot be deduced from the context, do not give an answer.\n",
    "  If there are image files in the given context, you show the images to the user.\n",
    "\n",
    "  #####\n",
    "  Here is the context: {context}\n",
    "  #####\n",
    "\n",
    "  #####\n",
    "  Here is chain of thought:\n",
    "  User: dresses for beach vacation\n",
    "  Assistant: Sure. Beach vacations are all about being free and close to nature. Here are suggestions: floral yellow shirt from brand X <image>, midi dress with fine blue prints from brand Y <image>\n",
    "  User: how about dresses for women in yellow\n",
    "  Assistant: Sure, here you go: midi dress from brand Y <image>, long floral gown from brand Z <image>\n",
    "  #####\n",
    "\n",
    "  You ask the follow up question to the user for relevant pairing suggestions like hat for beach, mufler for cold weather etc.\n",
    "  \"\"\"\n",
    "  human = \"{query}\"\n",
    "\n",
    "  prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "  chat = create_chat_client()\n",
    "\n",
    "  chain = prompt | chat\n",
    "  response = chain.invoke(\n",
    "      {\n",
    "          \"context\" : results,\n",
    "          \"domain\" : config['domain'],\n",
    "          \"query\": query,\n",
    "      }\n",
    "  )\n",
    "  return response.content\n",
    "\n",
    "def get_reranked_query_results(query):\n",
    "  model = HuggingFaceCrossEncoder(model_name=config['rerank_model'])\n",
    "\n",
    "  compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "  \n",
    "  compression_retriever = ContextualCompressionRetriever(\n",
    "      base_compressor=compressor,\n",
    "      base_retriever=get_retriever()\n",
    "  )\n",
    "\n",
    "  compressed_docs = compression_retriever.invoke(query)\n",
    "\n",
    "  return compressed_docs\n",
    "\n",
    "def rag_pipeline(user_question):\n",
    "\n",
    "  # Retrieve relevant documents from the vector store based on the user's question.\n",
    "  retrieved_documents = get_reranked_query_results(user_question)\n",
    "\n",
    "  # Extract the page content from the retrieved documents and join them into a single context string.\n",
    "  formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_documents)\n",
    "\n",
    "  # Call the large language model with the user's question and the formatted context.\n",
    "  answer = get_llm_response(user_question, formatted_context, config['domain'])\n",
    "  return answer\n",
    "\n",
    "def get_answer(user_query, history):\n",
    "  # return the answer to user query. This function is useful in interaction with Gradio UI\n",
    "  return rag_pipeline(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2bb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunked_data = []\n",
    "folder_path = Path(config['data_path'])\n",
    "# folder_path = Path(\"/content/data/\")\n",
    "\n",
    "# Step 1: Generate chunks from the dataset\n",
    "if config['refresh_vector_store'] == 'Y':\n",
    "    chunked_data = get_data_chunks(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30362634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create embeddings and put them into a vector store\n",
    "embedding_model = get_embeddings_model()\n",
    "vector_store = create_vector_store(chunked_data, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9861bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "query = \"party dress for women\"\n",
    "results = get_reranked_query_results(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fad53fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For a **party dress for women**, top options include cocktail dresses, sequin dresses, and elegant gowns in a variety of styles, colors, and lengths. Popular choices feature bold colors, shimmering fabrics, and unique details like ruffles, lace, or asymmetrical cuts[1][2][3][5][6].\\n\\nKey styles to consider:\\n- **Cocktail dresses**: Typically knee-length or midi, made from luxe fabrics like silk, satin, or sequins. These are versatile for most parties and semi-formal events[1][2][4][6].\\n- **Sequin and embellished dresses**: Perfect for evening parties, these add glamour and stand out in photos[1][4][6].\\n- **Long gowns or maxi dresses**: Ideal for formal parties or evening events, especially in black, metallic, or jewel tones[1][3][5].\\n- **Trendy cuts**: Off-shoulder, halter neck, and wrap styles are popular for a modern look[1][5][6].\\n- **Bold colors and prints**: Red, emerald, gold, and classic black are always in style, but bright hues and patterns are trending for daytime or summer parties[1][2][3][7].\\n\\nFor a complete party look, pair your dress with statement accessories like a clutch, heels, and bold jewelry[7]. If the event is outdoors or in cooler weather, consider a chic blazer or cape as a cover-up[2].\\n\\nWould you like suggestions for accessories or shoes to pair with your party dress?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_response(query, results, config['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40e337a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here are some popular options for **party dresses for women**:\\n\\n- **Sequin dresses** and **satin dresses** are trending for evening events, offering a glamorous and eye-catching look[2][6].\\n- **Little black dresses** remain a classic choice for cocktail parties, while bold colors like red, green, and yellow are great for making a statement[1][2][3][4].\\n- **Styles** range from mini and midi lengths to long gowns, with options like off-the-shoulder, halter, and sleeveless cuts[3][4][5].\\n- **Unique details** such as ruffles, asymmetrical designs, and embellishments like beading or lace add elegance and personality to your outfit[1][5].\\n- **Jumpsuits** in satin or bold colors are also a chic alternative for party wear[2].\\n\\nWould you like suggestions for pairing accessories, such as statement earrings or a clutch, to complete your party look?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(\"party dresses for women\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75fe66d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/6035x_495mg87nyn6pl05mp40000gq/T/ipykernel_78882/4142205462.py:4: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=500),\n",
      "/Users/RajivGaba/aiml_projects/.venv/lib/python3.13/site-packages/gradio/chat_interface.py:323: UserWarning: The type of the gr.Chatbot does not match the type of the gr.ChatInterface.The type of the gr.ChatInterface, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m iface = gr.ChatInterface(\n\u001b[32m      2\u001b[39m     get_answer,\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mtype\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     cache_examples=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43miface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshare\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/.venv/lib/python3.13/site-packages/gradio/blocks.py:2734\u001b[39m, in \u001b[36mBlocks.launch\u001b[39m\u001b[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[39m\n\u001b[32m   2732\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2733\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m         share_url = \u001b[43mnetworking\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_tunnel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_port\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshare_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshare_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshare_server_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshare_server_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshare_server_tls_certificate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshare_server_tls_certificate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2741\u001b[39m         parsed_url = urlparse(share_url)\n\u001b[32m   2742\u001b[39m         \u001b[38;5;28mself\u001b[39m.share_url = urlunparse(\n\u001b[32m   2743\u001b[39m             (\u001b[38;5;28mself\u001b[39m.share_server_protocol,) + parsed_url[\u001b[32m1\u001b[39m:]\n\u001b[32m   2744\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/.venv/lib/python3.13/site-packages/gradio/networking.py:59\u001b[39m, in \u001b[36msetup_tunnel\u001b[39m\u001b[34m(local_host, local_port, share_token, share_server_address, share_server_tls_certificate)\u001b[39m\n\u001b[32m     50\u001b[39m     remote_port = \u001b[38;5;28mint\u001b[39m(remote_port)\n\u001b[32m     51\u001b[39m tunnel = Tunnel(\n\u001b[32m     52\u001b[39m     remote_host,\n\u001b[32m     53\u001b[39m     remote_port,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     share_server_tls_certificate,\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m address = \u001b[43mtunnel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m address\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/.venv/lib/python3.13/site-packages/gradio/tunneling.py:114\u001b[39m, in \u001b[36mTunnel.start_tunnel\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart_tunnel\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.download_binary()\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28mself\u001b[39m.url = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_start_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBINARY_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.url\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/.venv/lib/python3.13/site-packages/gradio/tunneling.py:154\u001b[39m, in \u001b[36mTunnel._start_tunnel\u001b[39m\u001b[34m(self, binary)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mself\u001b[39m.proc = subprocess.Popen(\n\u001b[32m    151\u001b[39m     command, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n\u001b[32m    152\u001b[39m )\n\u001b[32m    153\u001b[39m atexit.register(\u001b[38;5;28mself\u001b[39m.kill)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_url_from_tunnel_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aiml_projects/.venv/lib/python3.13/site-packages/gradio/tunneling.py:176\u001b[39m, in \u001b[36mTunnel._read_url_from_tunnel_stream\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proc.stdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m line = line.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m line == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "iface = gr.ChatInterface(\n",
    "    get_answer,\n",
    "    type = 'messages',\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    "    textbox=gr.Textbox(placeholder=\"Help me find a formal shirt..\", container=False, scale=7),\n",
    "    title=\"ClosetAI - Fashion Studio\",\n",
    "    description=\"Ask ClosetAI anything about fashion products on Mytra\",\n",
    "    theme=\"glass\",\n",
    "    # examples=[\"Blazers for men\", \"Party dresses for women\", \"Athleisure\" , \"Sports attire\" ],\n",
    "    cache_examples=True,\n",
    ")\n",
    "iface.launch(share=True, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b24cf60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! Type 'exit' to quit.\n",
      "{'input': 'what is the currency of india', 'history': [HumanMessage(content='what is the currency of india', additional_kwargs={}, response_metadata={}), AIMessage(content='The currency of India is the **Indian rupee**, with the symbol **₹** and the currency code **INR**[1][2][3][4][5]. \\n\\nThe Indian rupee is subdivided into 100 **paise** (singular: paisa)[1][4]. The Reserve Bank of India is responsible for issuing and regulating the currency[1]. Indian banknotes are commonly found in denominations of 5, 10, 20, 50, 100, 200, 500, and 2000 rupees, while coins are available in 1, 2, 5, and 10 rupees[2]. \\n\\nThe rupee has a long history, first introduced in its modern form in 1540, and is one of the oldest continuously used currencies in the world[3].', additional_kwargs={}, response_metadata={})], 'text': 'The currency of India is the **Indian rupee**, with the symbol **₹** and the currency code **INR**[1][2][3][4][5]. \\n\\nThe Indian rupee is subdivided into 100 **paise** (singular: paisa)[1][4]. The Reserve Bank of India is responsible for issuing and regulating the currency[1]. Indian banknotes are commonly found in denominations of 5, 10, 20, 50, 100, 200, 500, and 2000 rupees, while coins are available in 1, 2, 5, and 10 rupees[2]. \\n\\nThe rupee has a long history, first introduced in its modern form in 1540, and is one of the oldest continuously used currencies in the world[3].'}\n",
      "{'input': 'best tourist place', 'history': [HumanMessage(content='what is the currency of india', additional_kwargs={}, response_metadata={}), AIMessage(content='The currency of India is the **Indian rupee**, with the symbol **₹** and the currency code **INR**[1][2][3][4][5]. \\n\\nThe Indian rupee is subdivided into 100 **paise** (singular: paisa)[1][4]. The Reserve Bank of India is responsible for issuing and regulating the currency[1]. Indian banknotes are commonly found in denominations of 5, 10, 20, 50, 100, 200, 500, and 2000 rupees, while coins are available in 1, 2, 5, and 10 rupees[2]. \\n\\nThe rupee has a long history, first introduced in its modern form in 1540, and is one of the oldest continuously used currencies in the world[3].', additional_kwargs={}, response_metadata={}), HumanMessage(content='best tourist place', additional_kwargs={}, response_metadata={}), AIMessage(content='The **Taj Mahal in Agra** is widely regarded as the best tourist place in India, recognized globally as a UNESCO World Heritage Site and one of the Seven Wonders of the World[1][2][4]. \\n\\nThe Taj Mahal is celebrated for its stunning white marble architecture and is a symbol of love, attracting millions of visitors each year[1][2]. It is often considered a must-visit destination for both domestic and international travelers due to its historical significance and breathtaking beauty[1][2][4].\\n\\nOther top tourist places in India include:\\n- **Jaipur** (the \"Pink City\") for its palaces and forts[1][2][4].\\n- **Goa** for its beaches and vibrant nightlife[1][2][4].\\n- **Varanasi** for spiritual experiences along the Ganges River[1][2][4].\\n- **Kerala** for its backwaters and natural beauty[1][2][3].\\n\\nEach of these destinations offers a unique experience, but the Taj Mahal remains the most iconic and frequently recommended site for visitors to India[1][2][4].', additional_kwargs={}, response_metadata={})], 'text': 'The **Taj Mahal in Agra** is widely regarded as the best tourist place in India, recognized globally as a UNESCO World Heritage Site and one of the Seven Wonders of the World[1][2][4]. \\n\\nThe Taj Mahal is celebrated for its stunning white marble architecture and is a symbol of love, attracting millions of visitors each year[1][2]. It is often considered a must-visit destination for both domestic and international travelers due to its historical significance and breathtaking beauty[1][2][4].\\n\\nOther top tourist places in India include:\\n- **Jaipur** (the \"Pink City\") for its palaces and forts[1][2][4].\\n- **Goa** for its beaches and vibrant nightlife[1][2][4].\\n- **Varanasi** for spiritual experiences along the Ganges River[1][2][4].\\n- **Kerala** for its backwaters and natural beauty[1][2][3].\\n\\nEach of these destinations offers a unique experience, but the Taj Mahal remains the most iconic and frequently recommended site for visitors to India[1][2][4].'}\n",
      "{'input': 'largest city by area', 'history': [HumanMessage(content='what is the currency of india', additional_kwargs={}, response_metadata={}), AIMessage(content='The currency of India is the **Indian rupee**, with the symbol **₹** and the currency code **INR**[1][2][3][4][5]. \\n\\nThe Indian rupee is subdivided into 100 **paise** (singular: paisa)[1][4]. The Reserve Bank of India is responsible for issuing and regulating the currency[1]. Indian banknotes are commonly found in denominations of 5, 10, 20, 50, 100, 200, 500, and 2000 rupees, while coins are available in 1, 2, 5, and 10 rupees[2]. \\n\\nThe rupee has a long history, first introduced in its modern form in 1540, and is one of the oldest continuously used currencies in the world[3].', additional_kwargs={}, response_metadata={}), HumanMessage(content='best tourist place', additional_kwargs={}, response_metadata={}), AIMessage(content='The **Taj Mahal in Agra** is widely regarded as the best tourist place in India, recognized globally as a UNESCO World Heritage Site and one of the Seven Wonders of the World[1][2][4]. \\n\\nThe Taj Mahal is celebrated for its stunning white marble architecture and is a symbol of love, attracting millions of visitors each year[1][2]. It is often considered a must-visit destination for both domestic and international travelers due to its historical significance and breathtaking beauty[1][2][4].\\n\\nOther top tourist places in India include:\\n- **Jaipur** (the \"Pink City\") for its palaces and forts[1][2][4].\\n- **Goa** for its beaches and vibrant nightlife[1][2][4].\\n- **Varanasi** for spiritual experiences along the Ganges River[1][2][4].\\n- **Kerala** for its backwaters and natural beauty[1][2][3].\\n\\nEach of these destinations offers a unique experience, but the Taj Mahal remains the most iconic and frequently recommended site for visitors to India[1][2][4].', additional_kwargs={}, response_metadata={}), HumanMessage(content='largest city by area', additional_kwargs={}, response_metadata={}), AIMessage(content='The **largest city in India by area is Delhi**, covering approximately **1,484 square kilometers**[1][3]. \\n\\nOther major Indian cities by area include:\\n- **Bengaluru**: about 714 km²[1][3]\\n- **Hyderabad**: about 650 km²[1][3]\\n- **Visakhapatnam**: about 640 km²[1]\\n- **Lucknow**: about 631 km²[1][3]\\n\\nDelhi is not only the national capital but also the most expansive city in terms of land area in India[1][3].', additional_kwargs={}, response_metadata={})], 'text': 'The **largest city in India by area is Delhi**, covering approximately **1,484 square kilometers**[1][3]. \\n\\nOther major Indian cities by area include:\\n- **Bengaluru**: about 714 km²[1][3]\\n- **Hyderabad**: about 650 km²[1][3]\\n- **Visakhapatnam**: about 640 km²[1]\\n- **Lucknow**: about 631 km²[1][3]\\n\\nDelhi is not only the national capital but also the most expansive city in terms of land area in India[1][3].'}\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "\n",
    "# Initialize the ChatPerplexity model\n",
    "llm = ChatPerplexity(model=\"sonar-pro\", temperature=0)\n",
    "\n",
    "# Define a prompt template with memory placeholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Set up memory to store conversation history\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Create the chain with memory\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Chat loop\n",
    "print(\"Chatbot: Hello! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = chain.invoke({\"input\": user_input})\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2892ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
