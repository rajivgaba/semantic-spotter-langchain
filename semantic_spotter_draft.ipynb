{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajivgaba/semantic-spotter-langchain/blob/main/semantic_spotter_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "\n",
        "! pip install -qU langchain-community pymupdf\n",
        "! pip install -qU langchain_huggingface\n",
        "! pip install -qU sentence-transformers\n",
        "! pip install -qU langchain-perplexity\n",
        "! pip install -qU faiss-cpu"
      ],
      "metadata": {
        "id": "wcbgs9IRy-Su"
      },
      "id": "wcbgs9IRy-Su",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import os, glob\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, CSVLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from sentence_transformers import CrossEncoder, util\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_perplexity import ChatPerplexity\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "import gradio as gr\n",
        "import kaggle\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "Cupxo0pnzDTM"
      },
      "id": "Cupxo0pnzDTM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c604d8a9",
      "metadata": {
        "id": "c604d8a9"
      },
      "outputs": [],
      "source": [
        "# Set the key for LLM API\n",
        "\n",
        "os.environ['PERPLEXITY_API_KEY'] = userdata.get('PPLX_API_KEY_2')\n",
        "\n",
        "# Set base configuration values\n",
        "\n",
        "config = {\n",
        "    'data_path' : '/Users/RajivGaba/aiml_projects/Semantic Spotter/Data/',\n",
        "    'chunk_size' : 512,\n",
        "    'chunk_overlap' : 80,\n",
        "    'vector_store_name' : \"faiss_myntra_db\",\n",
        "    'embedding_model' : 'all-MiniLM-L6-v2',\n",
        "    'refresh_vector_store' : 'Y',\n",
        "    'PPLX_API_KEY' : os.getenv('PERPLEXITY_API_KEY'),\n",
        "    'domain' : 'fashion',\n",
        "    'chat_model' : \"sonar-pro\",\n",
        "    'rerank_model' : 'BAAI/bge-reranker-base'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453649c6",
      "metadata": {
        "id": "453649c6"
      },
      "outputs": [],
      "source": [
        "# Download dataset from kaggle using kaggle API\n",
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "# api.dataset_download_files('promptcloud/myntra-e-commerce-product-data-november-2023', path='./data/', unzip=True)\n",
        "# api.dataset_download_files(\"ronakbokaria/myntra-products-dataset\", path='./data/', unzip=True)\n",
        "api.dataset_download_files(\"djagatiya/myntra-fashion-product-dataset\", path='./data/', unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4751e1",
      "metadata": {
        "id": "de4751e1"
      },
      "outputs": [],
      "source": [
        "# Define reusable functions\n",
        "\n",
        "def get_data_chunks(folder_path):\n",
        "  \"\"\"\n",
        "  This function will create chunks from the files present in the dataset.\n",
        "  This takes directory where the files exists. Basis the type of file i.e.\n",
        "  PDF, CSV or text, a loader is initialised and chunks are created from\n",
        "  content of the data\n",
        "  \"\"\"\n",
        "  # loader = PyMuPDFLoader(pdf_file)\n",
        "  # documents = loader.load()\n",
        "\n",
        "  all_documents = []\n",
        "\n",
        "  # Loop through all files in the folder\n",
        "  for file_path in folder_path.iterdir():\n",
        "      if file_path.suffix.lower() == \".pdf\":\n",
        "          loader = PyMuPDFLoader(str(file_path))\n",
        "      elif file_path.suffix.lower() == \".csv\":\n",
        "          loader = CSVLoader(str(file_path))\n",
        "      elif file_path.suffix.lower() == \".txt\":\n",
        "          loader = TextLoader(str(file_path))\n",
        "      else:\n",
        "          continue  # Skip unsupported file types\n",
        "\n",
        "      # Load and append documents\n",
        "      documents = loader.load()\n",
        "      all_documents.extend(documents)\n",
        "\n",
        "  # chunking/splitting\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=config['chunk_size'],\n",
        "      chunk_overlap=config['chunk_overlap'],\n",
        "      strip_whitespace=True,\n",
        "      separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "  )\n",
        "  text_chunks = text_splitter.split_documents(documents=all_documents)\n",
        "  return text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_model():\n",
        "  embedding_model = HuggingFaceEmbeddings(\n",
        "      model_name=config['embedding_model'],\n",
        "      show_progress=True,\n",
        "      multi_process=True,\n",
        "      model_kwargs={'device': 'cuda'}\n",
        "  )\n",
        "  return embedding_model"
      ],
      "metadata": {
        "id": "6rJ1G-G_Noys"
      },
      "id": "6rJ1G-G_Noys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(text_chunks, embedding_model):\n",
        "  if config['refresh_vector_store'] == 'Y' or not os.path.exists(config['vector_store_name']):\n",
        "      vector_store = FAISS.from_documents(text_chunks, embedding_model)\n",
        "      vector_store.save_local(config['vector_store_name'])\n",
        "  else:\n",
        "      vector_store = FAISS.load_local(config['vector_store_name'], embedding_model, allow_dangerous_deserialization=True)\n",
        "  return vector_store"
      ],
      "metadata": {
        "id": "nlJZPSvWNtL3"
      },
      "id": "nlJZPSvWNtL3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chat_client():\n",
        "  return ChatPerplexity(\n",
        "      temperature=0,\n",
        "      pplx_api_key=config['PPLX_API_KEY'], # Pass the API key explicitly\n",
        "      model=config['chat_model']\n",
        "  )"
      ],
      "metadata": {
        "id": "jwPjVHETNw6z"
      },
      "id": "jwPjVHETNw6z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_retriever(top_k=5):\n",
        "  retriever = vector_store.as_retriever(search_kwargs={'k': top_k})\n",
        "  return retriever"
      ],
      "metadata": {
        "id": "7zNXGfHJN053"
      },
      "id": "7zNXGfHJN053",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(query, results, domain, history=None):\n",
        "  system = \"\"\"\n",
        "  You are a helpful assistant in {domain} domain.\n",
        "  Using the information contained in the context, give an answer to the question.\n",
        "  Respond only to the question asked, response should be concise and relevant to the question.\n",
        "  Provide the number of the source document when relevant.\n",
        "  If the answer cannot be deduced from the context, do not give an answer.\n",
        "\n",
        "  #####\n",
        "  Here is the chain of thought:\n",
        "  User: dresses for beach vacation\n",
        "  Assistant: Sure. Beach vacations are all about being free and close to nature. Here are suggestions: floral yellow shirt from brand X <image>, midi dress with fine blue prints from brand Y <image>\n",
        "  User: how about dresses for women in yellow\n",
        "  Assistant: Sure, here you go: midi dress from brand Y <image>, long floral gown from brand Z <image>\n",
        "  #####\n",
        "\n",
        "  You ask the follow up question to the user for relevant pairing suggestions like hat for beach, mufler for cold weather etc.\n",
        "  \"\"\"\n",
        "  human = \"{query}\"\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", system),\n",
        "           (\"human\", human)\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  chat = create_chat_client()\n",
        "\n",
        "  chain = prompt | chat\n",
        "  response = chain.invoke(\n",
        "      {\n",
        "          \"context\" : results,\n",
        "          \"domain\" : config['domain'],\n",
        "          \"query\": query,\n",
        "      }\n",
        "  )\n",
        "  return response.content"
      ],
      "metadata": {
        "id": "OOkbCVJJN4Nz"
      },
      "id": "OOkbCVJJN4Nz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reranked_query_results(query):\n",
        "  model = HuggingFaceCrossEncoder(model_name=config['rerank_model'])\n",
        "\n",
        "  compressor = CrossEncoderReranker(model=model, top_n=5)\n",
        "\n",
        "  compression_retriever = ContextualCompressionRetriever(\n",
        "      base_compressor=compressor,\n",
        "      base_retriever=get_retriever()\n",
        "  )\n",
        "\n",
        "  compressed_docs = compression_retriever.invoke(query)\n",
        "\n",
        "  return compressed_docs"
      ],
      "metadata": {
        "id": "0pjFPHuXN7n1"
      },
      "id": "0pjFPHuXN7n1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(user_question, history):\n",
        "\n",
        "  print(f\"{history}\")\n",
        "\n",
        "  # Retrieve relevant documents from the vector store based on the user's question.\n",
        "  retrieved_documents = get_reranked_query_results(user_question)\n",
        "\n",
        "  # Extract the page content from the retrieved documents and join them into a single context string.\n",
        "  formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_documents)\n",
        "\n",
        "  # Call the large language model with the user's question and the formatted context.\n",
        "  answer = get_llm_response(user_question, formatted_context, config['domain'])\n",
        "  return answer"
      ],
      "metadata": {
        "id": "U7UiTasvN9KW"
      },
      "id": "U7UiTasvN9KW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_with_history_pipeline(new_user_message, gradio_history):\n",
        "    # --- STEP 1: Load Gradio history into LangChain Memory ---\n",
        "    # Gradio sends history in the format: [[user_msg, bot_msg], [user_msg, bot_msg], ...]\n",
        "    # We must reset the memory and rebuild it for each new call,\n",
        "    # as Gradio manages the state outside of LangChain's memory object.\n",
        "\n",
        "    # Clear memory from previous (potentially different user) sessions\n",
        "    memory.clear()\n",
        "\n",
        "    # Convert Gradio format to LangChain Messages and load into memory\n",
        "    for message_pair in gradio_history:\n",
        "        # message_pair[0] is User message, message_pair[1] is Assistant message\n",
        "        user_text = message_pair[0]\n",
        "        ai_text = message_pair[1]\n",
        "\n",
        "        # Add the conversation turn to the memory object\n",
        "        memory.chat_memory.add_user_message(user_text)\n",
        "        memory.chat_memory.add_ai_message(ai_text)\n",
        "\n",
        "\n",
        "    # --- STEP 2: RAG Logic (Retrieve context using the new question AND the history) ---\n",
        "    # Your current retrieval logic needs the new question (new_user_message)\n",
        "    # AND the context from the conversation history to perform better retrieval.\n",
        "\n",
        "    # Create the full query string by joining the last few messages\n",
        "    # This helps the retriever find context relevant to the full conversation.\n",
        "    if gradio_history:\n",
        "        last_turn = f\"User: {gradio_history[-1][0]} | Assistant: {gradio_history[-1][1]}\"\n",
        "        full_query = f\"{last_turn} | New Question: {new_user_message}\"\n",
        "    else:\n",
        "        full_query = new_user_message\n",
        "\n",
        "    # Run your vector store retrieval using 'full_query'\n",
        "    # final_retrieved_docs = retriever.invoke(full_query)\n",
        "\n",
        "    # Format the retrieved documents into the context string\n",
        "    # formatted_context = \"\\n\".join([doc.page_content for doc in final_retrieved_docs])\n",
        "\n",
        "    # NOTE: You'll need to update your final prompt template to include {context}\n",
        "    # and pass the formatted_context into the invoke call.\n",
        "\n",
        "\n",
        "    # --- STEP 3: Invoke the Chain ---\n",
        "    # The chain automatically pulls 'chat_history' from memory,\n",
        "    # and inserts 'input' and 'context' (if you update the prompt)\n",
        "    response = conversation_chain.invoke(\n",
        "        {\"input\": new_user_message,\n",
        "         #\"context\": formatted_context # Include this if you update the prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # The final memory state is now updated internally by the chain.\n",
        "    # The Gradio interface will update its displayed history based on the returned string.\n",
        "    return response['text']\n"
      ],
      "metadata": {
        "id": "9TqTGOAYLDiu"
      },
      "id": "9TqTGOAYLDiu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Initialize your Perplexity model (or any other ChatModel)\n",
        "# NOTE: Replace with your actual LLM instance (e.g., ChatPerplexity)\n",
        "llm = ChatPerplexity(model=\"sonar-medium-online\", perplexity_api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "# 1. Define the Memory Component\n",
        "# The memory stores the history (user/assistant messages)\n",
        "# It uses 'chat_history' as the key to insert into the prompt template\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# 2. Define the Prompt Template\n",
        "# Use MessagesPlaceholder to tell LangChain where to inject the list of messages\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful fashion assistant. Use the following context to answer the user's question. If you don't know the answer, say so. Keep the conversation flowing.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"), # <-- THIS is where memory goes\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Define a simple Chain (or your RAG chain wrapper)\n",
        "# For this simplified example, we'll use a basic LLMChain\n",
        "conversation_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False, # Set to True to see the full prompt, including history\n",
        "    memory=memory, # Pass the memory object to the chain\n",
        ")"
      ],
      "metadata": {
        "id": "LXNa9IczM-8t"
      },
      "id": "LXNa9IczM-8t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e890ce3",
      "metadata": {
        "id": "9e890ce3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chunked_data = []\n",
        "    # folder_path = Path(config['data_path'])\n",
        "    folder_path = Path(\"/content/data/\")\n",
        "\n",
        "    # Step 1: Generate chunks from the dataset\n",
        "    if config['refresh_vector_store'] == 'Y':\n",
        "        chunked_data = get_data_chunks(folder_path)\n",
        "\n",
        "    # Step 2: Create embeddings and put them into a vector store\n",
        "    embedding_model = get_embeddings_model()\n",
        "    vector_store = create_vector_store(chunked_data, embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.ChatInterface(\n",
        "    fn=rag_pipeline,\n",
        "    type = 'messages',\n",
        "    chatbot=gr.Chatbot(height=500),\n",
        "    textbox=gr.Textbox(placeholder=\"Help me find a formal shirt..\", container=False, scale=7),\n",
        "    title=\"ClosetAI - Fashion Studio\",\n",
        "    description=\"Ask ClosetAI anything about fashion products on Mytra\",\n",
        "    theme=\"glass\",\n",
        "    # examples=[\"Blazers for men\", \"Party dresses for women\", \"Athleisure\" , \"Sports attire\" ],\n",
        "    cache_examples=True,\n",
        ")\n",
        "iface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "423jyFmOw3VK"
      },
      "id": "423jyFmOw3VK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "id": "30QuW6iQ56YL"
      },
      "id": "30QuW6iQ56YL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}