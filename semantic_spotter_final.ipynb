{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6454164,
          "sourceType": "datasetVersion",
          "datasetId": 3726549
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajivgaba/semantic-spotter-langchain/blob/main/semantic_spotter_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ¨ Problem Statement âœ¨\n",
        "\n",
        "*Fashion Search AI* ðŸ‘—ðŸ‘–ðŸ‘•ðŸ‘ŸðŸ‘  : Create a generative search system capable of searching a plethora of product descriptions to find and recommend appropriate choices against a user query.\n",
        "\n",
        "**Author:** Rajiv Gaba <br>\n",
        "***LinkedIn:*** https://www.linkedin.com/in/rajiv-gaba/ <br>\n",
        "***GitHub:*** https://github.com/rajivgaba <br><br>"
      ],
      "metadata": {
        "id": "_F_jZKLIRLNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\n",
        "\n",
        "This cell installs all the necessary Python packages using `pip`. The `-qU` flags ensure a quiet installation and upgrade of packages. The required libraries include:\n",
        "\n",
        "- `langchain-community`: Provides integrations with various external resources.\n",
        "- `langchain_huggingface`: Enables the use of Hugging Face models within LangChain.\n",
        "- `sentence-transformers`: For generating sentence embeddings.\n",
        "- `langchain-perplexity`: Integration with the Perplexity AI API.\n",
        "- `faiss-cpu`: A library for efficient similarity search and clustering of dense vectors.\n",
        "- `gradio`: For creating a user interface for the application.\n",
        "- `kaggle`: To interact with the Kaggle API for dataset download."
      ],
      "metadata": {
        "id": "rvehrbG6PSQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "\n",
        "! pip install -qU langchain-community\n",
        "! pip install -qU langchain_huggingface\n",
        "! pip install -qU sentence-transformers\n",
        "! pip install -qU langchain-perplexity\n",
        "! pip install -qU faiss-cpu\n",
        "! pip install -qU gradio\n",
        "! pip install -qU kaggle"
      ],
      "metadata": {
        "id": "5d9c81a9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:37:32.422752Z",
          "iopub.execute_input": "2025-10-01T14:37:32.423118Z",
          "iopub.status.idle": "2025-10-01T14:39:39.450835Z",
          "shell.execute_reply.started": "2025-10-01T14:37:32.423094Z",
          "shell.execute_reply": "2025-10-01T14:39:39.44996Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter Warnings\n",
        "\n",
        "This cell imports the `warnings` module and sets a filter to ignore all warnings. This is often done to keep the output clean, but it's important to be aware that this might hide potentially useful information about issues in the code."
      ],
      "metadata": {
        "id": "t5jSRiD5PvX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:39:39.451822Z",
          "iopub.execute_input": "2025-10-01T14:39:39.45205Z",
          "iopub.status.idle": "2025-10-01T14:39:39.456781Z",
          "shell.execute_reply.started": "2025-10-01T14:39:39.452027Z",
          "shell.execute_reply": "2025-10-01T14:39:39.456057Z"
        },
        "id": "-0MczOHgO6e9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries\n",
        "\n",
        "This cell imports all the necessary libraries for the RAG pipeline and Gradio interface. These include:\n",
        "\n",
        "- `os`, `glob`, `Path`: For interacting with the file system.\n",
        "- `PyMuPDFLoader`, `CSVLoader`, `TextLoader`: Loaders from `langchain-community` for reading different file types.\n",
        "- `RecursiveCharacterTextSplitter`: For splitting text into smaller chunks.\n",
        "- `HuggingFaceEmbeddings`: For generating embeddings using Hugging Face models.\n",
        "- `FAISS`: For creating and loading a FAISS vector store.\n",
        "- `CrossEncoder`, `util`: From `sentence_transformers` for potential use in reranking (although `HuggingFaceCrossEncoder` is used later).\n",
        "- `ChatPromptTemplate`, `MessagesPlaceholder`: For creating chat prompts.\n",
        "- `ChatPerplexity`: For interacting with the Perplexity AI chat model.\n",
        "- `ContextualCompressionRetriever`: For compressing and reranking retrieved documents.\n",
        "- `ConversationBufferMemory`: For managing conversation history.\n",
        "- `CrossEncoderReranker`: For reranking documents using a cross-encoder model.\n",
        "- `HuggingFaceCrossEncoder`: A specific cross-encoder implementation from Hugging Face.\n",
        "- `HumanMessage`, `AIMessage`: For representing messages in a conversation.\n",
        "- `gradio`: For building the user interface.\n",
        "- `PIL.Image`: For handling images."
      ],
      "metadata": {
        "id": "qQcppjhrPkmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import os, glob\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, CSVLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from sentence_transformers import CrossEncoder, util\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_perplexity import ChatPerplexity\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import gradio as gr\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "c604d8a9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:39:39.458883Z",
          "iopub.execute_input": "2025-10-01T14:39:39.459566Z",
          "iopub.status.idle": "2025-10-01T14:40:19.432997Z",
          "shell.execute_reply.started": "2025-10-01T14:39:39.45954Z",
          "shell.execute_reply": "2025-10-01T14:40:19.432444Z"
        },
        "outputId": "3c9cd84a-857f-4185-8556-03e1df095c09"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-10-01 14:39:59.666540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759329600.042728      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759329600.146096      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set LLM Keys\n",
        "\n",
        "This cell sets the API key for the Perplexity AI model based on the execution environment (Kaggle, Colab, or Local). It retrieves the key from environment variables or secrets managers."
      ],
      "metadata": {
        "id": "_s-WBzEjPqrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set LLM keys from secret manager / local environment\n",
        "\n",
        "if os.path.exists('/kaggle'):\n",
        "    platform = 'Kaggle'\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    print(f\"Using kaggle secrets to get keys\")\n",
        "    os.environ['PERPLEXITY_API_KEY']  = user_secrets.get_secret(\"PPLX_API_KEY_2\")\n",
        "elif os.path.exists('/content'):\n",
        "    platform = 'Colab'\n",
        "    from google.colab import userdata\n",
        "    print(f\"Using Google colab secrets to get keys\")\n",
        "    os.environ['PERPLEXITY_API_KEY'] = userdata.get('PPLX_API_KEY_2')\n",
        "else:\n",
        "    platform = 'Local'\n",
        "    import dotenv\n",
        "    dotenv.load_dotenv()\n",
        "    print(f\"Using local env secrets to get keys\")\n",
        "    os.environ['PERPLEXITY_API_KEY'] = os.getenv('PPLX_API_KEY')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.433757Z",
          "iopub.execute_input": "2025-10-01T14:40:19.434728Z",
          "iopub.status.idle": "2025-10-01T14:40:19.538952Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.434698Z",
          "shell.execute_reply": "2025-10-01T14:40:19.538189Z"
        },
        "id": "n8Rr-O7aO6e-",
        "outputId": "7966e82b-3f53-4d30-83a4-d625f93e079c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using kaggle secrets to get keys\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Configuration\n",
        "\n",
        "This cell defines a Python dictionary `config` that holds various configuration parameters for the application, such as data paths, chunk sizes, model names, and API keys."
      ],
      "metadata": {
        "id": "x8L0axvYP0FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a base configuration\n",
        "\n",
        "config = {\n",
        "    'data_path' : '/kaggle/input/myntra-fashion-product-dataset/',\n",
        "    'images_path' : '/kaggle/input/myntra-fashion-product-dataset/images/',\n",
        "    'chunk_size' : 512,\n",
        "    'chunk_overlap' : 80,\n",
        "    'vector_store_name' : \"faiss_myntra_db\",\n",
        "    'embedding_model' : 'all-MiniLM-L6-v2',\n",
        "    'refresh_vector_store' : 'N',\n",
        "    'PPLX_API_KEY' : os.getenv('PERPLEXITY_API_KEY'),\n",
        "    'domain' : 'fashion',\n",
        "    'chat_model' : \"sonar-pro\",\n",
        "    'rerank_model' : 'BAAI/bge-reranker-base',\n",
        "    'platform' : platform\n",
        "}"
      ],
      "metadata": {
        "id": "9e6ea496",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.53973Z",
          "iopub.execute_input": "2025-10-01T14:40:19.540478Z",
          "iopub.status.idle": "2025-10-01T14:40:19.54408Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.540452Z",
          "shell.execute_reply": "2025-10-01T14:40:19.543588Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset\n",
        "\n",
        "This cell downloads the Myntra fashion product dataset from Kaggle using the Kaggle API if the notebook is not being run on the Kaggle platform."
      ],
      "metadata": {
        "id": "UAwfeiS-P5m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset if not running on kaggle notebook\n",
        "\n",
        "if config['platform'] != 'Kaggle':\n",
        "    # Download dataset from kaggle using kaggle API\n",
        "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "    # api.dataset_download_files('promptcloud/myntra-e-commerce-product-data-november-2023', path='./data/', unzip=True)\n",
        "    # api.dataset_download_files(\"ronakbokaria/myntra-products-dataset\", path='./data/', unzip=True)\n",
        "    api.dataset_download_files(\"djagatiya/myntra-fashion-product-dataset\", path='./data/', unzip=True)"
      ],
      "metadata": {
        "id": "ca535a39",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.544621Z",
          "iopub.execute_input": "2025-10-01T14:40:19.5448Z",
          "iopub.status.idle": "2025-10-01T14:40:19.569472Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.544787Z",
          "shell.execute_reply": "2025-10-01T14:40:19.568945Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Metadata to Documents\n",
        "\n",
        "This function `add_metadata_to_documents` takes a list of documents and extracts specific information (image URL, product ID, name, category, price, color, brand, rating count, average rating) from the document content to add as metadata. This enriched metadata can be useful for filtering and displaying information later."
      ],
      "metadata": {
        "id": "KYX3Y3zhP-_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that will enhance the metadata of the documents\n",
        "\n",
        "def add_metadata_to_documents(documents):\n",
        "    \"\"\"\n",
        "    Adds image URL and other attributes from page content to the metadata of each document.\n",
        "    \"\"\"\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            image_url = doc.page_content.split(\"\\n\")[6].split(' ')[1]\n",
        "            doc.metadata['image_url'] = image_url\n",
        "            pid = doc.page_content.split('\\n')[0].split()[1]\n",
        "            doc.metadata['image_path_local'] = config['images_path'] + pid + \".jpg\"\n",
        "            doc.metadata['p_id'] = pid\n",
        "            doc.metadata['product_name'] = doc.page_content.split(\"\\n\")[1].split(' ')[1]\n",
        "            doc.metadata['product_category'] = doc.page_content.split(\"\\n\")[2].split(' ')[1]\n",
        "            doc.metadata['price'] = doc.page_content.split(\"\\n\")[3].split(' ')[1]\n",
        "            doc.metadata['color'] = doc.page_content.split(\"\\n\")[4].split(' ')[1]\n",
        "            doc.metadata['brand'] = doc.page_content.split(\"\\n\")[5].split(' ')[1]\n",
        "            doc.metadata['rating_count'] = doc.page_content.split(\"\\n\")[7].split(' ')[1]\n",
        "            doc.metadata['avg_rating'] = doc.page_content.split(\"\\n\")[8].split(' ')[1]\n",
        "        except (IndexError, AttributeError):\n",
        "            # Handle cases where the image URL might not be present or in a different format\n",
        "            doc.metadata['image_url'] = None\n",
        "            doc.metadata['image_path_local'] = None\n",
        "            doc.metadata['p_id'] = None\n",
        "            doc.metadata['product_name'] = None\n",
        "            doc.metadata['product_category'] = None\n",
        "            doc.metadata['price'] = None\n",
        "            doc.metadata['price'] = None\n",
        "            doc.metadata['brand'] = None\n",
        "            doc.metadata['rating_count'] = None\n",
        "            doc.metadata['avg_rating'] = None\n",
        "    return documents"
      ],
      "metadata": {
        "id": "26c37152",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.570134Z",
          "iopub.execute_input": "2025-10-01T14:40:19.570324Z",
          "iopub.status.idle": "2025-10-01T14:40:19.588862Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.570309Z",
          "shell.execute_reply": "2025-10-01T14:40:19.588174Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Data Chunks\n",
        "\n",
        "This function `get_data_chunks` reads files from a specified folder path, loads their content using appropriate loaders (PDF, CSV, or Text), adds metadata using the `add_metadata_to_documents` function, and then splits the documents into smaller chunks using `RecursiveCharacterTextSplitter`."
      ],
      "metadata": {
        "id": "CdI_-BlyQCm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_chunks(folder_path):\n",
        "  \"\"\"\n",
        "  This function will create chunks from the files present in the dataset.\n",
        "  This takes directory where the files exists. Basis the type of file i.e.\n",
        "  PDF, CSV or text, a loader is initialised and chunks are created from\n",
        "  content of the data\n",
        "  \"\"\"\n",
        "  # loader = PyMuPDFLoader(pdf_file)\n",
        "  # documents = loader.load()\n",
        "\n",
        "  all_documents = []\n",
        "\n",
        "  # Loop through all files in the folder\n",
        "  for file_path in folder_path.iterdir():\n",
        "      if file_path.suffix.lower() == \".pdf\":\n",
        "          loader = PyMuPDFLoader(str(file_path))\n",
        "      elif file_path.suffix.lower() == \".csv\":\n",
        "          loader = CSVLoader(str(file_path))\n",
        "      elif file_path.suffix.lower() == \".txt\":\n",
        "          loader = TextLoader(str(file_path))\n",
        "      else:\n",
        "          continue  # Skip unsupported file types\n",
        "\n",
        "      # Load and append documents\n",
        "      documents = loader.load()\n",
        "      documents_with_metadata = add_metadata_to_documents(documents)\n",
        "      all_documents.extend(documents_with_metadata)\n",
        "\n",
        "  # chunking/splitting\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=config['chunk_size'],\n",
        "      chunk_overlap=config['chunk_overlap'],\n",
        "      strip_whitespace=True,\n",
        "      separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "  )\n",
        "  text_chunks = text_splitter.split_documents(documents=documents_with_metadata)\n",
        "  return text_chunks"
      ],
      "metadata": {
        "id": "f67bc650",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.589665Z",
          "iopub.execute_input": "2025-10-01T14:40:19.590199Z",
          "iopub.status.idle": "2025-10-01T14:40:19.610683Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.590177Z",
          "shell.execute_reply": "2025-10-01T14:40:19.610162Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Embeddings Model\n",
        "\n",
        "This function `get_embeddings_model` initializes and returns a `HuggingFaceEmbeddings` model with the specified model name and device. Embeddings are numerical representations of text that capture semantic meaning."
      ],
      "metadata": {
        "id": "AHasmLqTQGkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_model():\n",
        "  embedding_model = HuggingFaceEmbeddings(\n",
        "      model_name=config['embedding_model'],\n",
        "      show_progress=True,\n",
        "      multi_process=True,\n",
        "      model_kwargs={'device': 'cuda'}\n",
        "  )\n",
        "  return embedding_model"
      ],
      "metadata": {
        "id": "808bad53",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.612679Z",
          "iopub.execute_input": "2025-10-01T14:40:19.613285Z",
          "iopub.status.idle": "2025-10-01T14:40:19.633306Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.613264Z",
          "shell.execute_reply": "2025-10-01T14:40:19.632816Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dad2cd19"
      },
      "source": [
        "### Create Vector Store\n",
        "\n",
        "This function `create_vector_store` creates or loads a FAISS vector store. If `refresh_vector_store` is 'Y' or the vector store doesn't exist, it creates a new one from the provided text chunks and embedding model and saves it locally. Otherwise, it loads an existing vector store. FAISS allows for efficient similarity search on the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(text_chunks, embedding_model):\n",
        "  if config['refresh_vector_store'] == 'Y' or not os.path.exists(config['vector_store_name']):\n",
        "      vector_store = FAISS.from_documents(text_chunks, embedding_model)\n",
        "      vector_store.save_local(config['vector_store_name'])\n",
        "  else:\n",
        "      vector_store = FAISS.load_local(config['vector_store_name'], embedding_model, allow_dangerous_deserialization=True)\n",
        "  return vector_store"
      ],
      "metadata": {
        "id": "290f41a6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.63395Z",
          "iopub.execute_input": "2025-10-01T14:40:19.634204Z",
          "iopub.status.idle": "2025-10-01T14:40:19.650637Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.634183Z",
          "shell.execute_reply": "2025-10-01T14:40:19.650065Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edac26ac"
      },
      "source": [
        "### Create Chat Client\n",
        "\n",
        "This function `create_chat_client` initializes and returns a `ChatPerplexity` object, which is used to interact with the Perplexity AI chat model. It sets the temperature, API key, and model name from the configuration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chat_client():\n",
        "  return ChatPerplexity(\n",
        "      temperature=0,\n",
        "      pplx_api_key=config['PPLX_API_KEY'], # Pass the API key explicitly\n",
        "      model=config['chat_model']\n",
        "  )"
      ],
      "metadata": {
        "id": "2ebb800e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.65113Z",
          "iopub.execute_input": "2025-10-01T14:40:19.651296Z",
          "iopub.status.idle": "2025-10-01T14:40:19.669284Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.651282Z",
          "shell.execute_reply": "2025-10-01T14:40:19.6688Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7913a79f"
      },
      "source": [
        "### Get Retriever\n",
        "\n",
        "This function `get_retriever` creates and returns a retriever from the FAISS vector store. The retriever is used to fetch relevant documents based on a user query. The `search_kwargs={'k': top_k}` specifies the number of top documents to retrieve."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_retriever(top_k=10):\n",
        "  retriever = vector_store.as_retriever(search_kwargs={'k': top_k})\n",
        "  return retriever"
      ],
      "metadata": {
        "id": "afbe7c02",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.669936Z",
          "iopub.execute_input": "2025-10-01T14:40:19.670162Z",
          "iopub.status.idle": "2025-10-01T14:40:19.690231Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.670145Z",
          "shell.execute_reply": "2025-10-01T14:40:19.689684Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e813d821"
      },
      "source": [
        "### Get Reranked Query Results\n",
        "\n",
        "This function `get_reranked_query_results` takes a user query, retrieves an initial set of documents using the retriever, and then uses a `HuggingFaceCrossEncoder` for reranking the retrieved documents. Reranking helps to improve the relevance of the retrieved documents by considering the query and document content together. The function returns the top `top_n` (set in the compressor) reranked documents."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reranked_query_results(query):\n",
        "    model = HuggingFaceCrossEncoder(model_name=config['rerank_model'])\n",
        "    compressor = CrossEncoderReranker(model=model, top_n=3)\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "      base_compressor=compressor,\n",
        "      base_retriever=get_retriever()\n",
        "    )\n",
        "    compressed_docs = compression_retriever.invoke(query)\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{compressed_docs}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return compressed_docs"
      ],
      "metadata": {
        "id": "762f2797",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.690928Z",
          "iopub.execute_input": "2025-10-01T14:40:19.691128Z",
          "iopub.status.idle": "2025-10-01T14:40:19.707558Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.691114Z",
          "shell.execute_reply": "2025-10-01T14:40:19.707048Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f05928f3"
      },
      "source": [
        "### Generate LLM Response\n",
        "\n",
        "This function `generate_llm_response` takes the user query and the reranked search results as input. It creates a `ChatPerplexity` client, defines a system message that provides context to the LLM based on the retrieved documents, and then uses a `ChatPromptTemplate` to format the prompt for the LLM. Finally, it invokes the LLM with the formatted prompt and returns the generated response."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llm_response(query, results):\n",
        "    llm = create_chat_client()\n",
        "    #if system_message is None:\n",
        "    system_message = f\"\"\"\n",
        "    You are a helpful AI assistant in fashion domain and expert in looking into given documents and find relevant products.\n",
        "    Do not give any product listing outside this context.\n",
        "    Context is give here:\n",
        "    #####\n",
        "    {results}\n",
        "    #####\n",
        "    If you don't know the answer, say so. Keep the conversation flowing.\n",
        "\n",
        "    #####\n",
        "    You extract brand, price, avg_rating, rating_count, image_url and image_path_local from the metadata\n",
        "    #####\n",
        "    \"\"\"\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"{system_message}\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt_template | llm\n",
        "    llm_response = chain.invoke(\n",
        "        {\"query\" : query,\n",
        "        \"system_message\" : system_message}\n",
        "    )\n",
        "    return llm_response"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.708109Z",
          "iopub.execute_input": "2025-10-01T14:40:19.708262Z",
          "iopub.status.idle": "2025-10-01T14:40:19.723901Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.70825Z",
          "shell.execute_reply": "2025-10-01T14:40:19.72344Z"
        },
        "id": "HgfWflaFO6fE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36f403fb"
      },
      "source": [
        "### RAG Pipeline\n",
        "\n",
        "This function `rag_pipeline` orchestrates the RAG (Retrieval Augmented Generation) process. It takes user input, retrieves and reranks relevant documents using `get_reranked_query_results`, formats the retrieved documents as context, and then generates a response using `generate_llm_response`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(user_input):\n",
        "    # Get reranked top results of the user query\n",
        "    retrieved_documents = get_reranked_query_results(user_input)\n",
        "    # formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_documents)\n",
        "    formatted_context = \"\\n\\n\".join( (str(doc.metadata) + doc.page_content) for doc in retrieved_documents)\n",
        "    print(\"*\"*80)\n",
        "    print(f\"\\n\\n\\n {formatted_context} \\n\\n\\n\")\n",
        "    print(\"*\"*80)\n",
        "    answer = generate_llm_response(user_input, formatted_context)\n",
        "    return answer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.724529Z",
          "iopub.execute_input": "2025-10-01T14:40:19.724696Z",
          "iopub.status.idle": "2025-10-01T14:40:19.740584Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.724682Z",
          "shell.execute_reply": "2025-10-01T14:40:19.740023Z"
        },
        "id": "bNtT45HgO6fE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd60ddd8"
      },
      "source": [
        "### Get Answer\n",
        "\n",
        "This function `get_answer` is a simple wrapper around the `rag_pipeline` function, taking a question as input and returning the final answer generated by the RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(question):\n",
        "    # Call the RAG pipeline to get the answer based on the user's question.\n",
        "    final_answer = rag_pipeline(question)\n",
        "    return final_answer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.741209Z",
          "iopub.execute_input": "2025-10-01T14:40:19.741475Z",
          "iopub.status.idle": "2025-10-01T14:40:19.761805Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.741453Z",
          "shell.execute_reply": "2025-10-01T14:40:19.761118Z"
        },
        "id": "sLpVkOcHO6fE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81e9c74"
      },
      "source": [
        "### Start Process and Set Path\n",
        "\n",
        "This cell sets the `folder_path` variable based on the platform the notebook is running on (Kaggle, Colab, or Local). This path is used to locate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the process and set path for dataset\n",
        "\n",
        "chunked_data = []\n",
        "\n",
        "if (config['platform']).lower() == 'kaggle':\n",
        "    folder_path = Path(\"/kaggle/input/myntra-fashion-product-dataset\")\n",
        "elif (config['platform']).lower() == 'colab':\n",
        "    folder_path = Path(\"/content/data/\")\n",
        "else:\n",
        "    folder_path = Path(config['data_path'])\n",
        "\n",
        "print(folder_path)"
      ],
      "metadata": {
        "id": "d9b98630",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.76249Z",
          "iopub.execute_input": "2025-10-01T14:40:19.762729Z",
          "iopub.status.idle": "2025-10-01T14:40:19.779425Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.762706Z",
          "shell.execute_reply": "2025-10-01T14:40:19.778701Z"
        },
        "outputId": "86373506-6d98-40fd-94a4-6555b7be1564"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/myntra-fashion-product-dataset\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9238880"
      },
      "source": [
        "### Create Vector Store\n",
        "\n",
        "This cell either generates chunks from the dataset and creates a new FAISS vector store or loads an existing one based on the `refresh_vector_store` configuration. This step prepares the vector store for efficient document retrieval."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if config['refresh_vector_store'] == 'Y':\n",
        "    # Generate chunks from the dataset\n",
        "    chunked_data = get_data_chunks(folder_path)\n",
        "\n",
        "    # Create embeddings and put them into a vector store\n",
        "    embedding_model = get_embeddings_model()\n",
        "    vector_store = create_vector_store(chunked_data, embedding_model)\n",
        "else:\n",
        "    vector_store = FAISS.load_local(\n",
        "        config['vector_store_name'],\n",
        "        get_embeddings_model(),\n",
        "        allow_dangerous_deserialization=True\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:19.780079Z",
          "iopub.execute_input": "2025-10-01T14:40:19.780262Z",
          "iopub.status.idle": "2025-10-01T14:40:28.799029Z",
          "shell.execute_reply.started": "2025-10-01T14:40:19.780247Z",
          "shell.execute_reply": "2025-10-01T14:40:28.798448Z"
        },
        "colab": {
          "referenced_widgets": [
            "9d84fdc51dc148f98454c1396d3f3bb6",
            "c4534ce76c984248a4540dd771d2db7d",
            "c8c581de9a1640b496feda8af7112c78",
            "41b53a08dfe14af99a22b81529dcf066",
            "8077dd8ea69b47389390ccecea295840",
            "a7787a410db541b89a54d813a05f374b",
            "126e0698685e4932adc02db7921040bb",
            "7a4a1d2806c745e88fdc41360c70c3c6",
            "f68bc31448304e78a3e00ee84ed46172",
            "d774afc8f4814cc6ac035eedae9d590d",
            "225b82e243de4d47a27dee2e475fa2ea"
          ]
        },
        "id": "_zffWfaKO6fF",
        "outputId": "fc35ce64-a2c6-4479-bd74-512c6777b1ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d84fdc51dc148f98454c1396d3f3bb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4534ce76c984248a4540dd771d2db7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8c581de9a1640b496feda8af7112c78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41b53a08dfe14af99a22b81529dcf066"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8077dd8ea69b47389390ccecea295840"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7787a410db541b89a54d813a05f374b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "126e0698685e4932adc02db7921040bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a4a1d2806c745e88fdc41360c70c3c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f68bc31448304e78a3e00ee84ed46172"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d774afc8f4814cc6ac035eedae9d590d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "225b82e243de4d47a27dee2e475fa2ea"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3b89c57"
      },
      "source": [
        "### User Input Example\n",
        "\n",
        "This cell sets an example `user_input` variable, which can be used for testing the RAG pipeline or the Gradio interface."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"party dresses for women\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:28.799814Z",
          "iopub.execute_input": "2025-10-01T14:40:28.800085Z",
          "iopub.status.idle": "2025-10-01T14:40:28.804096Z",
          "shell.execute_reply.started": "2025-10-01T14:40:28.800062Z",
          "shell.execute_reply": "2025-10-01T14:40:28.803368Z"
        },
        "id": "KJJmlCbQO6fF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4038da54"
      },
      "source": [
        "### Gradio Chat Interface\n",
        "\n",
        "This function `gradio_chat_interface` is designed to be used with Gradio. It takes user input, calls the `rag_pipeline` to get the LLM response, cleans the response text, and attempts to retrieve and display product images from the metadata of the top 3 retrieved documents."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "\n",
        "def gradio_chat_interface(user_input):\n",
        "    results = get_reranked_query_results(user_input)\n",
        "    response = generate_llm_response(user_input, results)\n",
        "\n",
        "    # Clean HTML entities\n",
        "    cleaned_text = html.unescape(response.content.replace('\"', '\"').replace('&', '&'))\n",
        "\n",
        "    # Get product images\n",
        "    images = []\n",
        "    for doc in results[:3]:\n",
        "        image_path = doc.metadata.get('image_path_local')\n",
        "        if image_path and os.path.exists(image_path):\n",
        "            try:\n",
        "                images.append(Image.open(image_path))\n",
        "            except:\n",
        "                images.append(None)\n",
        "        else:\n",
        "            images.append(None)\n",
        "\n",
        "    while len(images) < 3:\n",
        "        images.append(None)\n",
        "\n",
        "    return cleaned_text, images[0], images[1], images[2]\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:28.804858Z",
          "iopub.execute_input": "2025-10-01T14:40:28.805149Z",
          "iopub.status.idle": "2025-10-01T14:40:28.852259Z",
          "shell.execute_reply.started": "2025-10-01T14:40:28.805129Z",
          "shell.execute_reply": "2025-10-01T14:40:28.851767Z"
        },
        "id": "Yscy5HSrO6fF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaedb5e3"
      },
      "source": [
        "### Create Gradio Interface\n",
        "\n",
        "This cell defines and launches the Gradio user interface. It creates a simple interface with a textbox for user input, a button to trigger the search, a textbox to display the LLM's recommendations, and image components to display product images. The `gradio_chat_interface` function is linked to the button click and textbox submission events."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Semantic Spotter\") as demo:\n",
        "    gr.Markdown(\"# ðŸ›ï¸ Semantic Spotter - Fashion Search\")\n",
        "\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(label=\"Search Query\", placeholder=\"party dresses for women\", scale=3)\n",
        "        submit_btn = gr.Button(\"Search\", variant=\"primary\", scale=1)\n",
        "\n",
        "    response_text = gr.Textbox(label=\"Recommendations\", lines=6, interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        img1 = gr.Image(label=\"Product 1\", height=200)\n",
        "        img2 = gr.Image(label=\"Product 2\", height=200)\n",
        "        img3 = gr.Image(label=\"Product 3\", height=200)\n",
        "\n",
        "    submit_btn.click(gradio_chat_interface, [user_input], [response_text, img1, img2, img3])\n",
        "    user_input.submit(gradio_chat_interface, [user_input], [response_text, img1, img2, img3])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-01T14:40:28.852864Z",
          "iopub.execute_input": "2025-10-01T14:40:28.853093Z",
          "iopub.status.idle": "2025-10-01T14:40:30.399183Z",
          "shell.execute_reply.started": "2025-10-01T14:40:28.853073Z",
          "shell.execute_reply": "2025-10-01T14:40:30.398446Z"
        },
        "id": "RFKN5_55O6fG",
        "outputId": "a6e827e8-c0fe-444e-ef2d-9364ac653fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://9c1dfbf1c13f5580a6.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://9c1dfbf1c13f5580a6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    }
  ]
}